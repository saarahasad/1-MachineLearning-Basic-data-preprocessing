{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the basic steps of data preprocessing that is extremely crucial to perform before beginning on your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing the libraries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img width=\"400\" height=\"400\" src=\"https://pbs.twimg.com/media/DqTK8dXX4AkkKy_.jpg\">\n",
    "\n",
    "### Numpy \n",
    "NumPy stands for ‘Numerical Python’ or ‘Numeric Python’. It is an open source module of Python which provides fast mathematical computation on arrays and matrices. \n",
    "\n",
    "### Pandas\n",
    "Similar to NumPy, Pandas is one of the most widely used python libraries in data science. It provides high-performance, easy to use structures and data analysis tools. Unlike NumPy library which provides objects for multi-dimensional arrays, Pandas provides in-memory 2d table object called Dataframe. It is like a spreadsheet with column names and row labels.\n",
    "\n",
    "### Matplotlib\n",
    "Matplotlib is a 2d plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments. Matplotlib can be used in Python scripts, Python and IPython shell, Jupyter Notebook, web application servers and GUI toolkits.\n",
    "\n",
    "More info: https://cloudxlab.com/blog/numpy-pandas-introduction/\n",
    "\n",
    "Make sure to read the official documentation too!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing the dataset\n",
    "\n",
    "### Steps: \n",
    "1. Import the dataset as a dataframe using pandas.\n",
    "2. Split the data into independent variables(X) and dependent variable(y) using iloc.\n",
    "3. The first parameter in iloc represents the number of rows and second parameter represents coloumns required. Eg. iloc[:, :-1] - selects all rows and all coloumns except for the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "\n",
      "y:\n",
      " ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "print(\"X:\\n\",X)\n",
    "print(\"\\ny:\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dealing with Missing data\n",
    "\n",
    "Missing values can be filled in with NaN, mean, median, the highest value of that coloumn.\n",
    "Imputer from the preprocessing sublibrary under the sklearn library could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "\n",
      "y:\n",
      " ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])\n",
    "print(\"X:\\n\",X)\n",
    "print(\"\\ny:\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since machines understand numbers more than they understand text. Categorical textual values are transformed to numerical values.\n",
    "\n",
    "From colomn 1 - France, Germany, Spain - can be transformed to - 0, 1, 2 -  numerical categories. However, since the cateogirs are given numbers as - 0, 1, 2 - the machine might misunderstand this to be ranking. It could misunderstand that Spain (category 2) is representing some value greater than Germany (category 1) or France (category 0). \n",
    "\n",
    "\n",
    "Thus, to solve this problem onehotencoding is performed.\n",
    "\n",
    "Must read:  https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "\n",
    "OneHotEncoding official documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[0 44.0 72000.0]\n",
      " [2 27.0 48000.0]\n",
      " [1 30.0 54000.0]\n",
      " [2 38.0 61000.0]\n",
      " [1 40.0 63777.77777777778]\n",
      " [0 35.0 58000.0]\n",
      " [2 38.77777777777778 52000.0]\n",
      " [0 48.0 79000.0]\n",
      " [1 50.0 83000.0]\n",
      " [0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "#labeling data numerically and Encoding the Independent Variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:,0] = labelencoder_X.fit_transform(X[:,0])\n",
    "print(\"X:\\n\",X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "\n",
      "y:\n",
      " [0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# OneHotEncoding the Independent Variables\n",
    "preprocess = make_column_transformer(([0],OneHotEncoder(categories='auto')),remainder=\"passthrough\")\n",
    "X = preprocess.fit_transform(X)\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "print(\"X:\\n\",X)\n",
    "print(\"\\ny:\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Splitting data into training and testing data\n",
    "\n",
    "The training set is a subset of your data on which your model will learn how to predict the dependent\n",
    "variable with the independent variables. The test set is the complimentary subset from the training set, on\n",
    "which you will evaluate your model to see if it manages to predict correctly the dependent variable with the\n",
    "independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:\n",
      " [[0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 37.0 67000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "\n",
      "y train:\n",
      " [1 1 1 0 1 0 0 1]\n",
      "\n",
      "X test:\n",
      " [[0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]]\n",
      "\n",
      "y test:\n",
      " [0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0,test_size=0.2)\n",
    "print(\"X train:\\n\",X_train)\n",
    "print(\"\\ny train:\\n\",y_train)\n",
    "print(\"\\nX test:\\n\",X_test)\n",
    "print(\"\\ny test:\\n\",y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Scaling\n",
    "\n",
    "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n",
    "\n",
    "If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n",
    "\n",
    "To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n",
    "\n",
    "There are two common methods to perform Feature Scaling.\n",
    "\n",
    "### 1. Standardisation:\n",
    "\n",
    "Standardisation replaces the values by their Z scores.\n",
    "\n",
    "<img width=\"100\" height=\"100\" src=\"https://cdn-images-1.medium.com/max/600/1*LysCPCvg0AzQenGoarL_hQ.png\">\n",
    "\n",
    "\n",
    "This redistributes the features with their mean μ = 0 and standard deviation σ =1 . sklearn.preprocessing.scale helps us implementing standardisation in python.\n",
    "\n",
    "### 2. Mean Normalisation:\n",
    "\n",
    "<img width=\"100\" height=\"100\" src=\"https://cdn-images-1.medium.com/max/900/1*fyK4gMQrfJKV5pmbXSrNbg.png\">\n",
    "\n",
    "\n",
    "This distribution will have values between -1 and 1 with μ=0.\n",
    "\n",
    "##### Note:\n",
    "\n",
    "##### Do we really have to apply Feature Scaling on the dummy variables?\n",
    "Yes, if you want to optimize the accuracy of your model predictions.\n",
    "No, if you want to keep the most interpretation as possible in your model.\n",
    "\n",
    "##### When should we use Standardization and Normalization?\n",
    "Generally you should normalize (normalization) when the data is normally distributed, and scale (standardization) when the data is not normally distributed. In doubt, you should go for standardization. However\n",
    "what is commonly done is that the two scaling methods are tested.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:\n",
      " [[-1.          2.64575131 -0.77459667  0.26306757  0.12381479]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25350148  0.46175632]\n",
      " [-1.         -0.37796447  1.29099445 -1.97539832 -1.53093341]\n",
      " [-1.         -0.37796447  1.29099445  0.05261351 -1.11141978]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64058505  1.7202972 ]\n",
      " [-1.         -0.37796447  1.29099445 -0.0813118  -0.16751412]\n",
      " [ 1.         -0.37796447 -0.77459667  0.95182631  0.98614835]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59788085 -0.48214934]]\n",
      "\n",
      "y train:\n",
      " [1 1 1 0 1 0 0 1]\n",
      "\n",
      "X test:\n",
      " [[-1.          2.64575131 -0.77459667 -1.45882927 -0.90166297]\n",
      " [-1.          2.64575131 -0.77459667  1.98496442  2.13981082]]\n",
      "\n",
      "y test:\n",
      " [0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "print(\"X train:\\n\",X_train)\n",
    "print(\"\\ny train:\\n\",y_train)\n",
    "print(\"\\nX test:\\n\",X_test)\n",
    "print(\"\\ny test:\\n\",y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
